{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5640f888",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Time series models\n",
    "try:\n",
    "    from fbprophet import Prophet\n",
    "    print(\"✓ Prophet imported\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Prophet not installed. Install with: pip install pystan==2.19.1.1 fbprophet\")\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered features\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/engineered_features.csv')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    print(f\"✓ Loaded engineered features: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Engineered features not found. Please run feature_engineering notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3241438",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7eaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "split_date = df['date'].quantile(0.8)\n",
    "df_train = df[df['date'] <= split_date].reset_index(drop=True)\n",
    "df_test = df[df['date'] > split_date].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training set: {len(df_train)} samples ({split_date.date()})\")\n",
    "print(f\"Test set: {len(df_test)} samples\")\n",
    "print(f\"\\nTrain-Test Split Ratio: {len(df_train)/len(df):.1%} / {len(df_test)/len(df):.1%}\")\n",
    "\n",
    "# Visualize split\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(df_train['date'], df_train['sales'], label='Training Data', color='steelblue', linewidth=1.5)\n",
    "ax.plot(df_test['date'], df_test['sales'], label='Test Data', color='coral', linewidth=1.5)\n",
    "ax.axvline(split_date, color='red', linestyle='--', alpha=0.7, label='Train-Test Split')\n",
    "ax.set_title('Train-Test Split', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/04_train_test_split.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n✓ Saved: 04_train_test_split.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78430405",
   "metadata": {},
   "source": [
    "## 3. Model 1: Facebook Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a887cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "prophet_train = df_train[['date', 'sales']].copy()\n",
    "prophet_train.columns = ['ds', 'y']\n",
    "\n",
    "# Initialize and train Prophet\n",
    "try:\n",
    "    print(\"Training Prophet model...\")\n",
    "    prophet_model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=10,\n",
    "        interval_width=0.95\n",
    "    )\n",
    "    \n",
    "    # Add regressors if available\n",
    "    if 'is_weekend' in df_train.columns:\n",
    "        prophet_train['is_weekend'] = df_train['is_weekend'].values\n",
    "        prophet_model.add_regressor('is_weekend')\n",
    "    \n",
    "    prophet_model.fit(prophet_train)\n",
    "    print(\"✓ Prophet model trained successfully\")\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    prophet_test = df_test[['date', 'sales']].copy()\n",
    "    prophet_test.columns = ['ds', 'y']\n",
    "    if 'is_weekend' in df_train.columns:\n",
    "        prophet_test['is_weekend'] = df_test['is_weekend'].values\n",
    "    \n",
    "    prophet_forecast = prophet_model.make_future_dataframe(periods=len(df_test))\n",
    "    if 'is_weekend' in df_train.columns:\n",
    "        prophet_forecast = prophet_forecast.merge(df[['date', 'is_weekend']], left_on='ds', right_on='date', how='left')\n",
    "        prophet_forecast = prophet_forecast.drop('date', axis=1)\n",
    "    \n",
    "    prophet_forecast = prophet_model.make_future_dataframe(periods=0)\n",
    "    prophet_forecast_test = prophet_model.predict(prophet_forecast.iloc[-len(df_test):])\n",
    "    \n",
    "    prophet_pred = prophet_forecast_test['yhat'].values\n",
    "    print(f\"✓ Prophet predictions made for {len(prophet_pred)} test samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Prophet training error: {str(e)}\")\n",
    "    prophet_pred = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc96409",
   "metadata": {},
   "source": [
    "## 4. Model 2: ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ARIMA model\n",
    "try:\n",
    "    print(\"Training ARIMA model...\")\n",
    "    # ARIMA(p,d,q) - commonly used: (5,1,2)\n",
    "    arima_model = ARIMA(df_train['sales'], order=(5, 1, 2))\n",
    "    arima_result = arima_model.fit()\n",
    "    print(\"✓ ARIMA model trained successfully\")\n",
    "    print(f\"\\nARIMA Summary:\")\n",
    "    print(arima_result.summary())\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    arima_pred = arima_result.get_forecast(steps=len(df_test)).predicted_mean.values\n",
    "    print(f\"✓ ARIMA predictions made for {len(arima_pred)} test samples\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ ARIMA training error: {str(e)}\")\n",
    "    arima_pred = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2511c0",
   "metadata": {},
   "source": [
    "## 5. Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for XGBoost\n",
    "# Select relevant features (excluding date and target)\n",
    "feature_cols = [col for col in df_train.columns if col not in ['date', 'sales']]\n",
    "\n",
    "X_train = df_train[feature_cols].fillna(0)\n",
    "y_train = df_train['sales']\n",
    "X_test = df_test[feature_cols].fillna(0)\n",
    "y_test = df_test['sales']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"XGBoost Features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(X_train_scaled)}\")\n",
    "print(f\"Test samples: {len(X_test_scaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "try:\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_test_scaled, y_test)],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"✓ XGBoost model trained successfully\")\n",
    "    \n",
    "    # Make predictions\n",
    "    xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "    print(f\"✓ XGBoost predictions made for {len(xgb_pred)} test samples\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ XGBoost training error: {str(e)}\")\n",
    "    xgb_pred = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd1e62",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "if prophet_pred is not None:\n",
    "    results.append(calculate_metrics(df_test['sales'], prophet_pred, 'Prophet'))\n",
    "\n",
    "if arima_pred is not None:\n",
    "    # Ensure predictions are positive (sales shouldn't be negative)\n",
    "    arima_pred_positive = np.maximum(arima_pred, 0)\n",
    "    results.append(calculate_metrics(df_test['sales'], arima_pred_positive, 'ARIMA'))\n",
    "\n",
    "if xgb_pred is not None:\n",
    "    results.append(calculate_metrics(df_test['sales'], xgb_pred, 'XGBoost'))\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model = results_df.loc[results_df['R²'].idxmax(), 'Model']\n",
    "print(f\"\\n✓ Best Model (by R² score): {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "models = [\n",
    "    ('Prophet', prophet_pred, axes[0]),\n",
    "    ('ARIMA', np.maximum(arima_pred, 0) if arima_pred is not None else None, axes[1]),\n",
    "    ('XGBoost', xgb_pred, axes[2])\n",
    "]\n",
    "\n",
    "for model_name, predictions, ax in models:\n",
    "    if predictions is not None:\n",
    "        ax.plot(df_test['date'], df_test['sales'], label='Actual', color='steelblue', linewidth=2, marker='o', markersize=4)\n",
    "        ax.plot(df_test['date'], predictions, label='Predicted', color='coral', linewidth=2, marker='s', markersize=4, alpha=0.7)\n",
    "        \n",
    "        # Calculate error for this model\n",
    "        mae = mean_absolute_error(df_test['sales'], predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(df_test['sales'], predictions))\n",
    "        \n",
    "        ax.set_title(f'{model_name} Predictions (MAE: ${mae:.2f}, RMSE: ${rmse:.2f})', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Sales')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/05_model_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Saved: 05_model_predictions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison chart\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'MAPE', 'R²']\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].barh(results_df['Model'], results_df[metric], color=['steelblue', 'coral', 'lightgreen'][:len(results_df)])\n",
    "    axes[i].set_title(metric, fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(metric)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(results_df[metric]):\n",
    "        axes[i].text(v + 0.01, j, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/06_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Saved: 06_model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7371e8",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65330ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xgb_pred is not None:\n",
    "    # Plot feature importance\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    top_features = feature_importance.head(15)\n",
    "    ax.barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'].values)\n",
    "    ax.set_xlabel('Feature Importance Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Top 15 Most Important Features (XGBoost)', fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(top_features['importance'].values):\n",
    "        ax.text(v + 0.002, i, f'{v:.4f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/07_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: 07_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b8d11",
   "metadata": {},
   "source": [
    "## 8. Save Models & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "import json\n",
    "\n",
    "results_dict = results_df.to_dict('records')\n",
    "with open('../outputs/model_comparison_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved: model_comparison_results.json\")\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df = pd.DataFrame({\n",
    "    'date': df_test['date'],\n",
    "    'actual_sales': df_test['sales']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
