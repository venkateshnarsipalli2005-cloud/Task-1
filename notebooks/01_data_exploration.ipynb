{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b374cb9",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series tools\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cc047",
   "metadata": {},
   "source": [
    "## 2. Load Historical Sales Data\n",
    "\n",
    "**Instructions**: Place your dataset in the `data/raw/` folder and update the file path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "# Adjust the file path and column names based on your dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/raw/sales_data.csv')\n",
    "    print(f\"✓ Data loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Data file not found. Please ensure your CSV is in data/raw/ folder.\")\n",
    "    print(\"\\nSupported datasets:\")\n",
    "    print(\"- Superstore Sales Dataset (Kaggle)\")\n",
    "    print(\"- Retail Sales Forecasting (Kaggle)\")\n",
    "    print(\"- Rossmann Store Sales (Kaggle)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1b000",
   "metadata": {},
   "source": [
    "## 3. Data Structure & Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"Data Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c08f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Percent': missing_percent.values\n",
    "})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20b24f",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Identify date column (adjust based on your dataset)\n",
    "# Common column names: 'Date', 'OrderDate', 'TransactionDate', 'DateTime'\n",
    "date_columns = [col for col in df_clean.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "print(f\"Potential date columns: {date_columns}\")\n",
    "\n",
    "# Convert date column to datetime\n",
    "if date_columns:\n",
    "    date_col = date_columns[0]\n",
    "    df_clean[date_col] = pd.to_datetime(df_clean[date_col], errors='coerce')\n",
    "    print(f\"✓ Converted '{date_col}' to datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "duplicates_before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "duplicates_removed = duplicates_before - len(df_clean)\n",
    "print(f\"✓ Removed {duplicates_removed} duplicate rows\")\n",
    "\n",
    "# Handle missing values in sales column\n",
    "sales_columns = [col for col in df_clean.columns if 'sales' in col.lower() or 'amount' in col.lower() or 'revenue' in col.lower()]\n",
    "print(f\"\\nPotential sales columns: {sales_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8801faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values strategy\n",
    "# For numerical columns: forward fill or mean\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        df_clean[col].fillna(df_clean[col].mean(), inplace=True)\n",
    "        print(f\"✓ Filled missing values in '{col}' with mean\")\n",
    "\n",
    "# For categorical columns: forward fill or mode\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "        print(f\"✓ Filled missing values in '{col}' with mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f57d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaning\n",
    "print(\"\\nCleaning Summary:\")\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(f\"Missing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types after cleaning:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793ea94",
   "metadata": {},
   "source": [
    "## 5. Time Series Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and set date column\n",
    "if date_columns:\n",
    "    date_col = date_columns[0]\n",
    "    sales_col = sales_columns[0] if sales_columns else df_clean.columns[2]\n",
    "    \n",
    "    # Create daily sales aggregation for time series\n",
    "    daily_sales = df_clean.groupby(date_col)[sales_col].sum().reset_index()\n",
    "    daily_sales.columns = ['date', 'sales']\n",
    "    daily_sales = daily_sales.sort_values('date')\n",
    "    daily_sales.set_index('date', inplace=True)\n",
    "    \n",
    "    print(f\"Daily sales data shape: {daily_sales.shape}\")\n",
    "    print(f\"Date range: {daily_sales.index.min()} to {daily_sales.index.max()}\")\n",
    "    print(f\"\\nFirst 10 days:\")\n",
    "    print(daily_sales.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b009d9",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be836ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall sales trends\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time series plot\n",
    "axes[0, 0].plot(daily_sales.index, daily_sales['sales'], linewidth=1.5, color='steelblue')\n",
    "axes[0, 0].set_title('Daily Sales Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Sales')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of sales\n",
    "axes[0, 1].hist(daily_sales['sales'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution of Daily Sales', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Sales Amount')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot for outlier detection\n",
    "axes[1, 0].boxplot(daily_sales['sales'])\n",
    "axes[1, 0].set_title('Box Plot of Sales (Outlier Detection)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Sales')\n",
    "\n",
    "# Monthly aggregation\n",
    "if date_col:\n",
    "    monthly_sales = df_clean.set_index(date_col)[sales_col].resample('M').sum()\n",
    "    axes[1, 1].bar(range(len(monthly_sales)), monthly_sales.values, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 1].set_title('Monthly Sales Aggregation', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Month')\n",
    "    axes[1, 1].set_ylabel('Total Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/01_sales_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: 01_sales_overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n=== SALES STATISTICS ===\")\n",
    "print(f\"Total Sales: ${daily_sales['sales'].sum():,.2f}\")\n",
    "print(f\"Average Daily Sales: ${daily_sales['sales'].mean():,.2f}\")\n",
    "print(f\"Median Daily Sales: ${daily_sales['sales'].median():,.2f}\")\n",
    "print(f\"Standard Deviation: ${daily_sales['sales'].std():,.2f}\")\n",
    "print(f\"Min Sales: ${daily_sales['sales'].min():,.2f}\")\n",
    "print(f\"Max Sales: ${daily_sales['sales'].max():,.2f}\")\n",
    "print(f\"Coefficient of Variation: {(daily_sales['sales'].std() / daily_sales['sales'].mean()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7570d4a",
   "metadata": {},
   "source": [
    "## 7. Seasonality & Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time components for analysis\n",
    "daily_sales['year'] = daily_sales.index.year\n",
    "daily_sales['month'] = daily_sales.index.month\n",
    "daily_sales['quarter'] = daily_sales.index.quarter\n",
    "daily_sales['day_of_week'] = daily_sales.index.dayofweek\n",
    "daily_sales['week_of_year'] = daily_sales.index.isocalendar().week\n",
    "\n",
    "print(\"✓ Added time components for seasonality analysis\")\n",
    "\n",
    "# Monthly patterns\n",
    "monthly_pattern = daily_sales.groupby('month')['sales'].agg(['mean', 'std', 'count'])\n",
    "monthly_pattern.index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "print(\"\\nMonthly Sales Pattern:\")\n",
    "print(monthly_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f209212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize seasonality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# By month\n",
    "daily_sales.groupby('month')['sales'].mean().plot(ax=axes[0, 0], marker='o', color='steelblue', linewidth=2)\n",
    "axes[0, 0].set_title('Average Sales by Month', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Average Sales')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# By day of week\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_avg = daily_sales.groupby('day_of_week')['sales'].mean()\n",
    "axes[0, 1].bar(range(len(day_avg)), day_avg.values, color='coral', edgecolor='black')\n",
    "axes[0, 1].set_xticks(range(7))\n",
    "axes[0, 1].set_xticklabels(day_names, rotation=45)\n",
    "axes[0, 1].set_title('Average Sales by Day of Week', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Average Sales')\n",
    "\n",
    "# By quarter\n",
    "daily_sales.groupby('quarter')['sales'].mean().plot(ax=axes[1, 0], marker='s', color='lightgreen', linewidth=2)\n",
    "axes[1, 0].set_title('Average Sales by Quarter', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Quarter')\n",
    "axes[1, 0].set_ylabel('Average Sales')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Year-over-year if multiple years\n",
    "if len(daily_sales['year'].unique()) > 1:\n",
    "    for year in sorted(daily_sales['year'].unique()):\n",
    "        year_data = daily_sales[daily_sales['year'] == year].groupby('month')['sales'].mean()\n",
    "        axes[1, 1].plot(year_data.index, year_data.values, marker='o', label=str(year), linewidth=2)\n",
    "    axes[1, 1].set_title('Year-over-Year Monthly Trends', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Month')\n",
    "    axes[1, 1].set_ylabel('Average Sales')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/02_seasonality_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: 02_seasonality_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb69fc",
   "metadata": {},
   "source": [
    "## 8. Key Findings & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== KEY FINDINGS ===\")\n",
    "print(f\"\\n1. Data Period: {daily_sales.index.min().date()} to {daily_sales.index.max().date()}\")\n",
    "print(f\"   Total Days: {len(daily_sales)} days\")\n",
    "\n",
    "# Trend direction\n",
    "first_half_avg = daily_sales['sales'].iloc[:len(daily_sales)//2].mean()\n",
    "second_half_avg = daily_sales['sales'].iloc[len(daily_sales)//2:].mean()\n",
    "trend_change = ((second_half_avg - first_half_avg) / first_half_avg) * 100\n",
    "print(f\"\\n2. Trend: {'Upward' if trend_change > 0 else 'Downward'} ({trend_change:+.1f}%)\")\n",
    "\n",
    "# Seasonality strength\n",
    "monthly_std = daily_sales.groupby('month')['sales'].mean().std()\n",
    "monthly_mean = daily_sales.groupby('month')['sales'].mean().mean()\n",
    "seasonality_strength = (monthly_std / monthly_mean) * 100\n",
    "print(f\"\\n3. Seasonality Strength: {seasonality_strength:.1f}%\")\n",
    "print(\"   (Higher values indicate stronger seasonal patterns)\")\n",
    "\n",
    "# Peak and low seasons\n",
    "peak_month = daily_sales.groupby('month')['sales'].mean().idxmax()\n",
    "low_month = daily_sales.groupby('month')['sales'].mean().idxmin()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "print(f\"\\n4. Peak Season: {month_names[peak_month-1]} | Low Season: {month_names[low_month-1]}\")\n",
    "\n",
    "# Day of week patterns\n",
    "peak_day = daily_sales.groupby('day_of_week')['sales'].mean().idxmax()\n",
    "print(f\"\\n5. Busiest Day: {day_names[peak_day]}\")\n",
    "\n",
    "# Volatility\n",
    "daily_returns = daily_sales['sales'].pct_change().dropna()\n",
    "volatility = daily_returns.std()\n",
    "print(f\"\\n6. Daily Sales Volatility: {volatility:.2%}\")\n",
    "print(\"\\n✓ Data exploration complete! Ready for feature engineering.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
