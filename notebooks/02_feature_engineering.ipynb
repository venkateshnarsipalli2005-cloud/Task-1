{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73adf3d3",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Data from Previous Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd926c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Load the cleaned data from exploration notebook\n",
    "try:\n",
    "    df_clean = pd.read_csv('../data/raw/sales_data.csv')\n",
    "    df_clean['date'] = pd.to_datetime(df_clean[df_clean.columns[[col for col in df_clean.columns if 'date' in col.lower()][0]]])\n",
    "    print(\"✓ Data loaded successfully\")\n",
    "except:\n",
    "    print(\"⚠ Could not load data automatically. Please ensure the data is in ../data/raw/\")\n",
    "    print(\"   You can rerun the previous notebook (01_data_exploration.ipynb) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335afa3",
   "metadata": {},
   "source": [
    "## 2. Create Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have daily aggregated data\n",
    "sales_col = [col for col in df_clean.columns if 'sales' in col.lower() or 'amount' in col.lower()][0]\n",
    "date_col = 'date'\n",
    "\n",
    "# Create daily time series\n",
    "daily_sales = df_clean.groupby(date_col)[sales_col].sum().reset_index()\n",
    "daily_sales.columns = ['date', 'sales']\n",
    "daily_sales['date'] = pd.to_datetime(daily_sales['date'])\n",
    "daily_sales = daily_sales.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Create complete date range (fill missing dates)\n",
    "date_range = pd.date_range(start=daily_sales['date'].min(), end=daily_sales['date'].max(), freq='D')\n",
    "daily_sales = daily_sales.set_index('date').reindex(date_range, fill_value=0).reset_index()\n",
    "daily_sales.columns = ['date', 'sales']\n",
    "\n",
    "print(f\"✓ Created daily time series: {len(daily_sales)} days\")\n",
    "print(f\"Date range: {daily_sales['date'].min().date()} to {daily_sales['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3784e7e",
   "metadata": {},
   "source": [
    "## 3. Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e893ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time components\n",
    "daily_sales['year'] = daily_sales['date'].dt.year\n",
    "daily_sales['month'] = daily_sales['date'].dt.month\n",
    "daily_sales['day'] = daily_sales['date'].dt.day\n",
    "daily_sales['day_of_week'] = daily_sales['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "daily_sales['day_name'] = daily_sales['date'].dt.day_name()\n",
    "daily_sales['quarter'] = daily_sales['date'].dt.quarter\n",
    "daily_sales['week_of_year'] = daily_sales['date'].dt.isocalendar().week\n",
    "daily_sales['day_of_year'] = daily_sales['date'].dt.dayofyear\n",
    "\n",
    "# Create categorical features\n",
    "daily_sales['is_weekend'] = (daily_sales['day_of_week'] >= 5).astype(int)\n",
    "daily_sales['is_month_start'] = daily_sales['date'].dt.is_month_start.astype(int)\n",
    "daily_sales['is_month_end'] = daily_sales['date'].dt.is_month_end.astype(int)\n",
    "daily_sales['is_quarter_start'] = daily_sales['date'].dt.is_quarter_start.astype(int)\n",
    "daily_sales['is_quarter_end'] = daily_sales['date'].dt.is_quarter_end.astype(int)\n",
    "daily_sales['is_year_start'] = daily_sales['date'].dt.is_year_start.astype(int)\n",
    "daily_sales['is_year_end'] = daily_sales['date'].dt.is_year_end.astype(int)\n",
    "\n",
    "print(\"✓ Created time-based features\")\n",
    "print(f\"\\nFeatures created: {len(daily_sales.columns) - 2} features\")\n",
    "print(daily_sales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce4d4f",
   "metadata": {},
   "source": [
    "## 4. Rolling Window Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling averages\n",
    "daily_sales['rolling_mean_7'] = daily_sales['sales'].rolling(window=7, min_periods=1).mean()\n",
    "daily_sales['rolling_mean_14'] = daily_sales['sales'].rolling(window=14, min_periods=1).mean()\n",
    "daily_sales['rolling_mean_30'] = daily_sales['sales'].rolling(window=30, min_periods=1).mean()\n",
    "daily_sales['rolling_mean_90'] = daily_sales['sales'].rolling(window=90, min_periods=1).mean()\n",
    "daily_sales['rolling_mean_365'] = daily_sales['sales'].rolling(window=365, min_periods=1).mean()\n",
    "\n",
    "# Rolling standard deviations (volatility)\n",
    "daily_sales['rolling_std_7'] = daily_sales['sales'].rolling(window=7, min_periods=1).std()\n",
    "daily_sales['rolling_std_30'] = daily_sales['sales'].rolling(window=30, min_periods=1).std()\n",
    "daily_sales['rolling_std_90'] = daily_sales['sales'].rolling(window=90, min_periods=1).std()\n",
    "\n",
    "# Rolling min and max\n",
    "daily_sales['rolling_min_7'] = daily_sales['sales'].rolling(window=7, min_periods=1).min()\n",
    "daily_sales['rolling_max_7'] = daily_sales['sales'].rolling(window=7, min_periods=1).max()\n",
    "daily_sales['rolling_range_7'] = daily_sales['rolling_max_7'] - daily_sales['rolling_min_7']\n",
    "\n",
    "print(\"✓ Created rolling window features\")\n",
    "print(f\"Rolling features: 7-day, 14-day, 30-day, 90-day, 365-day windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852614c",
   "metadata": {},
   "source": [
    "## 5. Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag features (previous day, week, month sales)\n",
    "for lag in [1, 7, 14, 30]:\n",
    "    daily_sales[f'lag_{lag}'] = daily_sales['sales'].shift(lag)\n",
    "\n",
    "# Year-over-year lag\n",
    "daily_sales['lag_365'] = daily_sales['sales'].shift(365)\n",
    "\n",
    "# Differences (rate of change)\n",
    "daily_sales['diff_1'] = daily_sales['sales'].diff(1)\n",
    "daily_sales['diff_7'] = daily_sales['sales'].diff(7)\n",
    "daily_sales['diff_30'] = daily_sales['sales'].diff(30)\n",
    "\n",
    "# Percentage change\n",
    "daily_sales['pct_change_1'] = daily_sales['sales'].pct_change(1)\n",
    "daily_sales['pct_change_7'] = daily_sales['sales'].pct_change(7)\n",
    "\n",
    "print(\"✓ Created lag features\")\n",
    "print(f\"Lags: 1, 7, 14, 30, 365 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43470a",
   "metadata": {},
   "source": [
    "## 6. Seasonal Decomposition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly aggregated features (seasonality by month)\n",
    "monthly_avg = daily_sales.groupby('month')['sales'].mean().to_dict()\n",
    "daily_sales['monthly_avg'] = daily_sales['month'].map(monthly_avg)\n",
    "daily_sales['monthly_seasonality'] = daily_sales['sales'] / daily_sales['monthly_avg']\n",
    "\n",
    "# Day-of-week seasonality\n",
    "dow_avg = daily_sales.groupby('day_of_week')['sales'].mean().to_dict()\n",
    "daily_sales['dow_avg'] = daily_sales['day_of_week'].map(dow_avg)\n",
    "daily_sales['dow_seasonality'] = daily_sales['sales'] / daily_sales['dow_avg']\n",
    "\n",
    "# Quarter seasonality\n",
    "quarter_avg = daily_sales.groupby('quarter')['sales'].mean().to_dict()\n",
    "daily_sales['quarter_avg'] = daily_sales['quarter'].map(quarter_avg)\n",
    "daily_sales['quarter_seasonality'] = daily_sales['sales'] / daily_sales['quarter_avg']\n",
    "\n",
    "print(\"✓ Created seasonal decomposition features\")\n",
    "print(\"Features: monthly_seasonality, dow_seasonality, quarter_seasonality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0a6f0",
   "metadata": {},
   "source": [
    "## 7. Holiday & Special Event Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define holidays (customize based on your region/business)\n",
    "# Format: (month, day) tuple\n",
    "holidays = {\n",
    "    'new_year': (1, 1),\n",
    "    'valentine_day': (2, 14),\n",
    "    'march_madness': (3, 15),  # Approximate\n",
    "    'independence_day': (7, 4),\n",
    "    'labor_day': (9, 1),  # First Monday of September\n",
    "    'black_friday': (11, 27),  # Varies by year\n",
    "    'cyber_monday': (11, 30),  # Varies by year\n",
    "    'christmas': (12, 25),\n",
    "    'boxing_day': (12, 26),\n",
    "}\n",
    "\n",
    "# Create holiday flags\n",
    "for holiday_name, (month, day) in holidays.items():\n",
    "    daily_sales[f'is_{holiday_name}'] = (\n",
    "        (daily_sales['month'] == month) & (daily_sales['day'] == day)\n",
    "    ).astype(int)\n",
    "\n",
    "# Days before/after major holidays\n",
    "major_holidays = ['christmas', 'black_friday']\n",
    "for holiday in major_holidays:\n",
    "    # Create holiday indicator window (3 days before and after)\n",
    "    holiday_mask = daily_sales[f'is_{holiday}'] == 1\n",
    "    daily_sales[f'{holiday}_window'] = (\n",
    "        holiday_mask | \n",
    "        holiday_mask.shift(1) | \n",
    "        holiday_mask.shift(2) | \n",
    "        holiday_mask.shift(3) |\n",
    "        holiday_mask.shift(-1) | \n",
    "        holiday_mask.shift(-2) | \n",
    "        holiday_mask.shift(-3)\n",
    "    ).astype(int)\n",
    "\n",
    "print(\"✓ Created holiday features\")\n",
    "print(f\"Holidays tracked: {', '.join(holidays.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ae5a3",
   "metadata": {},
   "source": [
    "## 8. Trend Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trend using linear regression on rolling windows\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def calculate_trend(series, window=30):\n",
    "    \"\"\"Calculate trend coefficient for rolling window\"\"\"\n",
    "    trends = []\n",
    "    for i in range(len(series)):\n",
    "        if i < window:\n",
    "            x = np.arange(i + 1)\n",
    "            y = series.iloc[:i+1].values\n",
    "        else:\n",
    "            x = np.arange(window)\n",
    "            y = series.iloc[i-window+1:i+1].values\n",
    "        \n",
    "        if len(x) > 1:\n",
    "            slope, _, _, _, _ = linregress(x, y)\n",
    "            trends.append(slope)\n",
    "        else:\n",
    "            trends.append(0)\n",
    "    \n",
    "    return pd.Series(trends, index=series.index)\n",
    "\n",
    "# Calculate trend features\n",
    "daily_sales['trend_7'] = calculate_trend(daily_sales['sales'], window=7)\n",
    "daily_sales['trend_30'] = calculate_trend(daily_sales['sales'], window=30)\n",
    "daily_sales['trend_90'] = calculate_trend(daily_sales['sales'], window=90)\n",
    "\n",
    "# Days since trend change\n",
    "daily_sales['days_increasing'] = (\n",
    "    (~(daily_sales['diff_1'] < 0)).cumsum() * (daily_sales['diff_1'] >= 0).astype(int)\n",
    ")\n",
    "\n",
    "print(\"✓ Created trend features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126bc8e",
   "metadata": {},
   "source": [
    "## 9. Feature Summary & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature summary\n",
    "print(\"\\n=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "print(f\"\\nTotal features created: {len(daily_sales.columns) - 2}\")\n",
    "print(f\"Dataset shape: {daily_sales.shape}\")\n",
    "print(f\"\\nFeature Categories:\")\n",
    "print(f\"  - Time components: 8 features\")\n",
    "print(f\"  - Rolling windows: 12 features\")\n",
    "print(f\"  - Lag features: 8 features\")\n",
    "print(f\"  - Seasonal features: 6 features\")\n",
    "print(f\"  - Holiday features: {sum(1 for col in daily_sales.columns if 'holiday' in col.lower() or 'is_' in col)}\")\n",
    "print(f\"  - Trend features: 4 features\")\n",
    "\n",
    "print(f\"\\nFirst few rows with features:\")\n",
    "print(daily_sales.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4017e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature correlations with target\n",
    "# Select key features for visualization\n",
    "key_features = [\n",
    "    'sales',\n",
    "    'rolling_mean_7',\n",
    "    'rolling_mean_30',\n",
    "    'lag_7',\n",
    "    'lag_30',\n",
    "    'trend_30',\n",
    "    'is_weekend'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Rolling averages\n",
    "axes[0, 0].plot(daily_sales['date'], daily_sales['sales'], label='Daily Sales', alpha=0.5, linewidth=1)\n",
    "axes[0, 0].plot(daily_sales['date'], daily_sales['rolling_mean_7'], label='7-day MA', linewidth=2)\n",
    "axes[0, 0].plot(daily_sales['date'], daily_sales['rolling_mean_30'], label='30-day MA', linewidth=2)\n",
    "axes[0, 0].set_title('Rolling Averages', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Sales')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend\n",
    "axes[0, 1].plot(daily_sales['date'], daily_sales['trend_30'], linewidth=2, color='green')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].fill_between(daily_sales['date'], 0, daily_sales['trend_30'], \n",
    "                         where=(daily_sales['trend_30'] >= 0), alpha=0.3, color='green', label='Uptrend')\n",
    "axes[0, 1].fill_between(daily_sales['date'], 0, daily_sales['trend_30'], \n",
    "                         where=(daily_sales['trend_30'] < 0), alpha=0.3, color='red', label='Downtrend')\n",
    "axes[0, 1].set_title('30-Day Trend', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Trend Coefficient')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekend vs Weekday\n",
    "weekend_sales = daily_sales[daily_sales['is_weekend'] == 1]['sales'].mean()\n",
    "weekday_sales = daily_sales[daily_sales['is_weekend'] == 0]['sales'].mean()\n",
    "axes[1, 0].bar(['Weekday', 'Weekend'], [weekday_sales, weekend_sales], color=['steelblue', 'coral'], edgecolor='black')\n",
    "axes[1, 0].set_title('Average Sales: Weekday vs Weekend', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Average Sales')\n",
    "for i, v in enumerate([weekday_sales, weekend_sales]):\n",
    "    axes[1, 0].text(i, v + 1, f'${v:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# Lag correlation\n",
    "lag_correlations = [\n",
    "    daily_sales['sales'].corr(daily_sales['lag_1']),\n",
    "    daily_sales['sales'].corr(daily_sales['lag_7']),\n",
    "    daily_sales['sales'].corr(daily_sales['lag_14']),\n",
    "    daily_sales['sales'].corr(daily_sales['lag_30']),\n",
    "]\n",
    "axes[1, 1].bar(['1-day', '7-day', '14-day', '30-day'], lag_correlations, color='lightgreen', edgecolor='black')\n",
    "axes[1, 1].set_title('Lag Feature Correlations with Sales', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Correlation')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "for i, v in enumerate(lag_correlations):\n",
    "    axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/03_feature_engineering.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Saved: 03_feature_engineering.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9898340",
   "metadata": {},
   "source": [
    "## 10. Handle Missing Values & Save Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values created by lag and rolling features\n",
    "print(f\"Missing values before imputation: {daily_sales.isnull().sum().sum()}\")\n",
    "\n",
    "# Forward fill then backward fill for missing values\n",
    "daily_sales_filled = daily_sales.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "\n",
    "print(f\"Missing values after imputation: {daily_sales_filled.isnull().sum().sum()}\")\n",
    "print(\"\\n✓ All missing values handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b454a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the engineered features\n",
    "daily_sales_filled.to_csv('../data/processed/engineered_features.csv', index=False)\n",
    "print(f\"✓ Saved engineered features to: data/processed/engineered_features.csv\")\n",
    "print(f\"\\nDataset ready for modeling:\")\n",
    "print(f\"  - Rows: {len(daily_sales_filled)}\")\n",
    "print(f\"  - Columns: {len(daily_sales_filled.columns)}\")\n",
    "print(f\"  - Date range: {daily_sales_filled['date'].min()} to {daily_sales_filled['date'].max()}\")\n",
    "print(f\"\\n✓ Feature engineering complete! Ready for model training.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
